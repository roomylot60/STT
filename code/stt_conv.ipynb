{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SST 내부 구조 파악\n",
    "- SST를 직접 생성하기 위해 코드를 참조 : [Ref.](https://www.kaggle.com/code/evanupham/stt-conv-nn)\n",
    "    * 참조 내용에서 Attention mechanism을 사용하고자 하였으나, 1 epoch에 걸리는 시간이 길어지고 효율이 좋지 않아 Gating mechanism을 사용하였다고 함\n",
    "    * Gated-TCN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import L\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import math\n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "\n",
    "# Temporal Convolutional Network (TCN) Model Definition\n",
    "\n",
    "class BSplineConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, num_knots, stride=1, padding=0, dilation=1, groups=1):\n",
    "        super(BSplineConv1d, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_knots = num_knots\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        \n",
    "        # Ensure the number of input and output channels is divisible by the number of groups\n",
    "        assert in_channels % groups == 0, 'in_channels must be divisible by groups'\n",
    "        assert out_channels % groups == 0, 'out_channels must be divisible by groups'\n",
    "        \n",
    "        # B-spline coefficients and knot vector\n",
    "        self.coefficients = nn.Parameter(torch.randn(out_channels, in_channels // groups, kernel_size))\n",
    "        self.knots = torch.linspace(0, 1, num_knots + kernel_size).unsqueeze(0).unsqueeze(0)\n",
    "        self.bias = nn.Parameter(torch.randn(out_channels))\n",
    "        \n",
    "    def bspline_basis(self, u, knots):\n",
    "        k = knots.size(-1) - u.size(-1) - 1\n",
    "        N = torch.zeros_like(u)\n",
    "        for i in range(k + 1):\n",
    "            d_i = knots[..., i:i + u.size(-1) + 1]\n",
    "            numer = (u - d_i[..., :-1])\n",
    "            denom = (d_i[..., 1:] - d_i[..., :-1])\n",
    "            basis = torch.where(denom != 0, numer / denom, torch.zeros_like(numer))\n",
    "            N += torch.prod(torch.clamp(basis, min=0), dim=-1)\n",
    "        return N\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Pad the input\n",
    "        x = F.pad(x, (self.padding, self.padding))\n",
    "        \n",
    "        # Unfold input into patches\n",
    "        batch_size, in_channels, length = x.size()\n",
    "        kernel_size_dilated = (self.kernel_size - 1) * self.dilation + 1\n",
    "        out_length = (length - kernel_size_dilated) // self.stride + 1\n",
    "        \n",
    "        unfolded = x.unfold(2, kernel_size_dilated, self.stride)\n",
    "        unfolded = unfolded[:, :, :, ::self.dilation]\n",
    "        \n",
    "        # Evaluate B-spline basis\n",
    "        u = torch.linspace(0, 1, self.kernel_size, device=x.device).unsqueeze(0).unsqueeze(0)\n",
    "        B = self.bspline_basis(u, self.knots.to(x.device))\n",
    "        \n",
    "        # Correctly reshape and expand B\n",
    "        B = B.squeeze().unsqueeze(0).unsqueeze(1).expand(self.coefficients.size(0), self.coefficients.size(1), self.coefficients.size(2), -1)\n",
    "        \n",
    "        # Manually compute B-spline weighted values\n",
    "        splined_weights = torch.einsum('oik, oikl -> oikl', self.coefficients, B)\n",
    "        \n",
    "        # Perform convolution using B-spline evaluated values directly\n",
    "        out = torch.zeros(batch_size, self.out_channels, out_length, device=x.device)\n",
    "        \n",
    "        for g in range(self.groups):\n",
    "            x_group = unfolded[:, g * (in_channels // self.groups):(g + 1) * (in_channels // self.groups), :, :]\n",
    "            weights_group = splined_weights[g * (self.out_channels // self.groups):(g + 1) * (self.out_channels // self.groups), :, :, :]\n",
    "            out[:, g * (self.out_channels // self.groups):(g + 1) * (self.out_channels // self.groups), :] = torch.einsum('bijk, oikl -> boj', x_group, weights_group)\n",
    "        \n",
    "        out += self.bias.unsqueeze(0).unsqueeze(2)\n",
    "        return out\n",
    "\n",
    "# Test the BSplineConv1d layer\n",
    "batch_size = 2\n",
    "in_channels = 4\n",
    "out_channels = 8\n",
    "kernel_size = 3\n",
    "num_knots = 4\n",
    "length = 16\n",
    "stride = 1\n",
    "padding = 1\n",
    "dilation = 1\n",
    "groups = 2\n",
    "\n",
    "x = torch.randn(batch_size, in_channels, length)\n",
    "model = BSplineConv1d(in_channels, out_channels, kernel_size, num_knots, stride, padding, dilation, groups)\n",
    "output = model(x)\n",
    "\n",
    "print(output.shape)\n",
    "\n",
    "    \n",
    "class DropBlock(nn.Module):\n",
    "    def __init__(self, drop_prob, block_size):\n",
    "        super(DropBlock, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.training or self.drop_prob == 0.0:\n",
    "            return x\n",
    "        gamma = self._compute_gamma(x)\n",
    "        mask = (torch.rand(x.shape[:2]) < gamma).float().to(x.device)\n",
    "        block_mask = self._compute_block_mask(mask, x.shape)\n",
    "        out = x * block_mask[:, :, None]\n",
    "        out = out * (block_mask.numel() / block_mask.sum())\n",
    "        return out\n",
    "\n",
    "    def _compute_block_mask(self, mask, shape):\n",
    "        batch_size, channels, length = shape\n",
    "        block_mask = F.max_pool1d(mask.unsqueeze(1), self.block_size, stride=1, padding=self.block_size // 2)\n",
    "        block_mask = 1 - block_mask.squeeze(1)\n",
    "        return block_mask\n",
    "\n",
    "    def _compute_gamma(self, x):\n",
    "        return self.drop_prob * (x.shape[2] ** 2) / (self.block_size ** 2) / (\n",
    "            (x.shape[2] - self.block_size + 1) ** 2)\n",
    "\n",
    "class DSC(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation_size, padding, dropout):\n",
    "        super(DSC, self).__init__()\n",
    "        self.depthwise = BSplineConv1d(in_channels, in_channels, kernel_size,\n",
    "                                   stride=1, padding=padding, dilation=dilation_size, groups=in_channels, num_knots=10)\n",
    "        self.pointwise = BSplineConv1d(in_channels, out_channels, 1, num_knots=10)\n",
    "        self.bn = nn.BatchNorm1d(out_channels)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.dropout = DropBlock(drop_prob=0.3, block_size=5)  # Using DropBlock instead of Dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.silu(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation_size, dropout):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        padding = (kernel_size - 1) * dilation_size // 2  # Ensure the output size is the same as the input size\n",
    "        self.conv = DSC(in_channels, out_channels, kernel_size, dilation_size, padding, dropout)\n",
    "        self.bn = nn.BatchNorm1d(out_channels)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.downsample = BSplineConv1d(in_channels, out_channels, 1,num_knots=10) if in_channels != out_channels else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv(x)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        return self.silu(out + residual)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure positional encodings are up to the same length as the input and have correct feature size\n",
    "        batch_size, seq_len, feature_size = x.shape\n",
    "        device = x.device\n",
    "\n",
    "        # Extend encoding to match batch size dynamically\n",
    "        encoding = self.encoding[:, :seq_len].repeat(batch_size, 1, 1).to(device)\n",
    "\n",
    "        if encoding.shape[2] != feature_size:\n",
    "            # Adjust feature dimension of encoding if it doesn't match\n",
    "            new_encoding = torch.zeros((batch_size, seq_len, feature_size), device=device)\n",
    "            min_features = min(feature_size, encoding.shape[2])\n",
    "            new_encoding[:, :, :min_features] = encoding[:, :, :min_features]\n",
    "            encoding = new_encoding\n",
    "\n",
    "        return x + encoding\n",
    "\n",
    "class ConvGate(nn.Module):\n",
    "    def __init__(self, channels, kernel_size=3):\n",
    "        super(ConvGate, self).__init__()\n",
    "        padding = kernel_size // 2\n",
    "        \n",
    "        # Convolution layers for gating\n",
    "        self.conv_gate = BSplineConv1d(channels, channels, kernel_size, padding=padding, num_knots=10)\n",
    "        self.conv_transform = BSplineConv1d(channels, channels, kernel_size, padding=padding, num_knots=10)\n",
    "        \n",
    "        # Attention mechanism components\n",
    "        self.attention_weights = nn.Parameter(torch.randn(channels, 1))  # Learnable attention weights\n",
    "\n",
    "        # Adaptive gate control\n",
    "        self.adaptive_scale = nn.Parameter(torch.randn(1, channels, 1))  # Learnable scaling for the gate\n",
    "        self.adaptive_bias = nn.Parameter(torch.randn(1, channels, 1))  # Learnable bias for the gate\n",
    "\n",
    "        # Activation and normalization\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.layer_norm = nn.LayerNorm(channels)  # Applying layer normalization per channel\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute the gate using sigmoid activation\n",
    "        gate = self.sigmoid(self.conv_gate(x) * self.adaptive_scale + self.adaptive_bias)\n",
    "        \n",
    "        # Apply the transformation\n",
    "        transformed = self.conv_transform(x)\n",
    "        \n",
    "        # Apply attention by multiplying with the learnable weights\n",
    "        attention = transformed * self.attention_weights\n",
    "\n",
    "        # Element-wise multiplication of the gate and the attention-applied transformation\n",
    "        gated_output = attention * gate\n",
    "        \n",
    "        # Adding a residual connection\n",
    "        gated_output = gated_output + x\n",
    "        \n",
    "        # Layer normalization\n",
    "        gated_output = self.layer_norm(gated_output.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "        return gated_output\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, num_features, num_classes, num_channels, kernel_size=3, dropout=0.2):\n",
    "        super(TCN, self).__init__()\n",
    "        self.pos_encoder = PositionalEncoding(num_features)\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_features if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [ConvGate(in_channels)]\n",
    "            layers += [ResidualBlock(in_channels, out_channels, kernel_size, dilation_size, dropout)]\n",
    "\n",
    "        self.tcn = nn.Sequential(*layers)\n",
    "        self.dropout = DropBlock(drop_prob=0.3, block_size=5)  # Using DropBlock instead of Dropout\n",
    "        self.layer_norm = nn.LayerNorm(num_channels[-1])\n",
    "        self.linear = nn.Linear(num_channels[-1], num_classes)\n",
    "        self.mish = nn.Mish()\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.tcn(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.mish(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Assuming DEVICE is set for GPU/CPU\n",
    "num_features = features_padded.shape[2]\n",
    "num_classes = tokenizer.get_vocab_size() + 1  # Includes space for the added padding token\n",
    "\n",
    "tcn_model = TCN(num_features, num_classes, [1024, 1024, 2048, 2048], kernel_size=3, dropout=0.25)\n",
    "tcn_model = tcn_model.to(DEVICE)\n",
    "\n",
    "# If multiple GPUs are available, wrap the model with DataParallel\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    tcn_model = nn.DataParallel(tcn_model)\n",
    "import torch.nn.init as init\n",
    "\n",
    "def initialize_weights(model, sparsity=0.9):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv1d):\n",
    "            # Sparse Kaiming He initialization\n",
    "            fan_out = init._calculate_correct_fan(m.weight, 'fan_out')\n",
    "            gain = init.calculate_gain('relu')\n",
    "            std_dev = gain / fan_out ** 0.5\n",
    "            with torch.no_grad():\n",
    "                m.weight.normal_(0, std_dev)\n",
    "                # Apply sparsity\n",
    "                mask = torch.rand(m.weight.shape) < sparsity  # Mask with desired sparsity\n",
    "                m.weight[mask] = 0\n",
    "            if m.bias is not None:\n",
    "                init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm1d):\n",
    "            init.constant_(m.weight, 1)\n",
    "            init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            # Sparse Xavier Glorot initialization\n",
    "            fan_in, fan_out = init._calculate_fan_in_and_fan_out(m.weight)\n",
    "            std_dev = (2.0 / (fan_in + fan_out))**0.5\n",
    "            with torch.no_grad():\n",
    "                m.weight.normal_(0, std_dev)\n",
    "                # Apply sparsity\n",
    "                mask = torch.rand(m.weight.shape) < sparsity  # Mask with desired sparsity\n",
    "                m.weight[mask] = 0\n",
    "            if m.bias is not None:\n",
    "                init.constant_(m.bias, 0)\n",
    "# Usage\n",
    "import torch.optim as optim\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# Define the base optimizer\n",
    "# Wrap it with Look-ahead\n",
    "class Lookahead(optim.Optimizer):\n",
    "    def __init__(self, optimizer, alpha=0.5, k=6):\n",
    "        if not isinstance(optimizer, optim.Optimizer):\n",
    "            raise TypeError(f\"{type(optimizer).__name__} is not an optimizer\")\n",
    "        self.optimizer = optimizer\n",
    "        self.defaults = optimizer.defaults\n",
    "        self.param_groups = optimizer.param_groups\n",
    "        self.state = optimizer.state\n",
    "        self.alpha = alpha\n",
    "        self.k = k\n",
    "        self.step_counter = 0\n",
    "\n",
    "        # Initialize the slow weights and ensure correct state initialization\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p not in self.state:\n",
    "                    continue\n",
    "                state = self.state[p]\n",
    "                if 'slow_buffer' not in state:\n",
    "                    state['slow_buffer'] = torch.clone(p.data).detach()\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = self.optimizer.step(closure)\n",
    "        self.step_counter += 1\n",
    "\n",
    "        if self.step_counter % self.k == 0:\n",
    "            for group in self.param_groups:\n",
    "                for p in group['params']:\n",
    "                    if p.grad is None:\n",
    "                        continue\n",
    "                    state = self.state[p]\n",
    "                    if 'slow_buffer' in state:\n",
    "                        slow = state['slow_buffer']\n",
    "                        fast = p.data\n",
    "                        fast_old = fast.clone()\n",
    "                        # Update slow and fast weights\n",
    "                        slow += self.alpha * (fast - slow)\n",
    "                        fast.copy_(slow)\n",
    "                        slow.copy_(fast_old)\n",
    "        return loss\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def add_param_group(self, param_group):\n",
    "        param_group = self.optimizer.add_param_group(param_group)\n",
    "        self.param_groups = self.optimizer.param_groups\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# initialize_weights(tcn_model)\n",
    "# model_state_dict = torch.load('/kaggle/working/tcn_model.pth')\n",
    "# tcn_model.load_state_dict(model_state_dict)\n",
    "\n",
    "ctc_loss = nn.CTCLoss(blank=0)  # Assuming the blank token is 0\n",
    "tcn_optimizer = optim.AdamW(tcn_model.parameters(), lr=0.0005)\n",
    "# Wrap the base optimizer with Lookahead\n",
    "tcn_optimizer = Lookahead(tcn_optimizer)\n",
    "# Helper function to decode the output from the network\n",
    "def decode_ctc_output(ctc_output, tokenizer):\n",
    "    decoded_sequence = []\n",
    "    max_indices = torch.argmax(ctc_output, dim=-1)  # Assuming the logits are the last dimension\n",
    "    prev_index = -1\n",
    "\n",
    "    for index in max_indices[0]:  # Assuming batch size of 1 for simplicity\n",
    "        if index != prev_index:  # 0 is the blank label in CTC\n",
    "            decoded_sequence.append(index.item())\n",
    "        prev_index = index\n",
    "\n",
    "    # Use the GPT-2 tokenizer to decode the sequence of token IDs into a string\n",
    "    decoded_text = tokenizer.decode(decoded_sequence)\n",
    "    return decoded_text\n",
    "\n",
    "\n",
    "\n",
    "def beam_search_decoder(data, k):\n",
    "    sequences = [([], 0)]\n",
    "    for row in data:\n",
    "        all_candidates = []\n",
    "        for seq, score in sequences:\n",
    "            for i, prob in enumerate(row):\n",
    "                candidate = (seq + [i], score - torch.log(prob))\n",
    "                all_candidates.append(candidate)\n",
    "        ordered = sorted(all_candidates, key=lambda x: x[1])\n",
    "        sequences = ordered[:k]\n",
    "    return sequences\n",
    "\n",
    "def decode_beam_search(ctc_output, tokenizer, beam_width=3):\n",
    "    \"\"\"Decodes using beam search from CTC output.\"\"\"\n",
    "    probs = torch.softmax(ctc_output, dim=2)\n",
    "    best_seqs = beam_search_decoder(probs[0], beam_width)\n",
    "    decoded_texts = []\n",
    "    for seq, score in best_seqs:\n",
    "        text = ''.join(tokenizer.index_word.get(i, '?') for i in seq if i != 0)  # Exclude the CTC blank label\n",
    "        decoded_texts.append((text, score))\n",
    "    return decoded_texts\n",
    "\n",
    "# Training Loop with Sample Inference\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from contextlib import nullcontext\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "id_to_token = {id: token for token, id in tokenizer.get_vocab().items()}\n",
    "\n",
    "def train_model(model, train_loader, optimizer, scheduler, num_epochs, tokenizer, device, repo_name, use_mixed_precision=False):\n",
    "    model.train()\n",
    "    scaler = GradScaler() if use_mixed_precision else None\n",
    "    total_params = list(model.named_parameters())\n",
    "    total_layers = len(total_params)\n",
    "    unfreeze_epochs = 7  # Number of epochs over which to unfreeze layers\n",
    "    unfreeze_interval = total_layers // unfreeze_epochs\n",
    "\n",
    "    # Initially freeze all layers\n",
    "    for name, param in total_params:\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if epoch < unfreeze_epochs:\n",
    "            # Gradual unfreezing: Unfreeze the next set of layers\n",
    "            layers_to_unfreeze = total_layers - unfreeze_interval * (epoch + 1)\n",
    "            print(f\"Epoch {epoch+1}: Unfreezing the following layers:\")\n",
    "            for i, (name, param) in enumerate(total_params):\n",
    "                if i >= layers_to_unfreeze:\n",
    "                    param.requires_grad = True\n",
    "                    print(f\" - {name} (unfrozen)\")\n",
    "                else:\n",
    "                    print(f\" - {name} (frozen)\")\n",
    "        else:\n",
    "            # After the initial unfreezing period, all layers are unfrozen\n",
    "            print(f\"Epoch {epoch+1}: All layers are unfrozen.\")\n",
    "\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        for features, targets in progress_bar:\n",
    "            features, targets = features.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if use_mixed_precision:\n",
    "                with autocast():\n",
    "                    loss = calculate_loss(model, features, targets)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss = calculate_loss(model, features, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            scheduler.step()\n",
    "            progress_bar.set_postfix({'loss': loss.item(), 'lr': scheduler.get_last_lr()[0]})\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Display average loss\n",
    "        average_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}: Average Loss = {average_loss}\")\n",
    "\n",
    "#         # Save and push checkpoint to the hub\n",
    "#         model.save_pretrained(f\"./{repo_name}\", epoch=epoch)\n",
    "#         model.push_to_hub(f\"{repo_name}\")\n",
    "\n",
    "        # Inference example to check model performance\n",
    "        model.eval()\n",
    "        with torch.no_grad(), (autocast() if use_mixed_precision else nullcontext()):\n",
    "            sample_output = model(features[:1])  # Adjusted to use the first batch for inference\n",
    "#             beam_decoded_texts = decode_beam_search(sample_output, tokenizer, beam_width=3)\n",
    "            decoded_text = decode_ctc_output(sample_output, id_to_token)\n",
    "            print(f\"Sample decoded text: {decoded_text}\")\n",
    "#             for text, score in beam_decoded_texts:\n",
    "#                 print(f\"Beam search decoded text: {text}, Score: {score}\")\n",
    "        model.train()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_loss(model, features, targets):\n",
    "    tcn_output = model(features)\n",
    "    output_lengths = torch.full((tcn_output.size(0),), tcn_output.size(1), dtype=torch.long)\n",
    "    target_lengths = torch.tensor([len(t[t != 0]) for t in targets], dtype=torch.long)\n",
    "    loss = ctc_loss(tcn_output.log_softmax(2).transpose(0, 1), targets, output_lengths, target_lengths)\n",
    "    l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "    l1_term = 0.00001  # Example regularization strength\n",
    "    loss += l1_term * l1_norm\n",
    "    return loss\n",
    "\n",
    "step_size_up = len(train_loader) * NUM_EPOCHS // 4  # Half of a cycle in terms of batch updates\n",
    "\n",
    "scheduler = CyclicLR(tcn_optimizer, base_lr=0.004, max_lr=0.013,\n",
    "                     step_size_up=step_size_up, step_size_down=None,\n",
    "                     mode='triangular2', cycle_momentum=False,\n",
    "                     scale_mode='cycle', gamma=0.5)\n",
    "# Start Training with sample inference\n",
    "\n",
    "\n",
    "# Usage\n",
    "train_model(tcn_model, train_loader, tcn_optimizer, scheduler, NUM_EPOCHS, tokenizer, DEVICE, \"UphamProjects/STT-Gated_TCN-12M\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 126] 지정된 모듈을 찾을 수 없습니다. Error loading \"c:\\USERS\\DHSMF\\ANACONDA3\\ENVS\\PY312\\Lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\USERS\\DHSMF\\ANACONDA3\\ENVS\\PY312\\Lib\\site-packages\\torch\\__init__.py:148\u001b[0m\n\u001b[0;32m    146\u001b[0m                 err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(ctypes\u001b[38;5;241m.\u001b[39mget_last_error())\n\u001b[0;32m    147\u001b[0m                 err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 148\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    150\u001b[0m     kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_preload_cuda_deps\u001b[39m(lib_folder, lib_name):\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 126] 지정된 모듈을 찾을 수 없습니다. Error loading \"c:\\USERS\\DHSMF\\ANACONDA3\\ENVS\\PY312\\Lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
